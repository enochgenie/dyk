{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Insight Generator\n",
    "Supports both pure LLM and evidence-based generation via OpenRouter API.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "import requests\n",
    "from prompt_templates import PromptTemplates, RegionSpecificPrompts\n",
    "from pubmed_integration import EvidenceRetriever\n",
    "\n",
    "\n",
    "class OpenRouterClient:\n",
    "    \"\"\"Client for OpenRouter API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize OpenRouter client.\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenRouter API key (or set OPENROUTER_API_KEY env var)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"OpenRouter API key required. Set OPENROUTER_API_KEY environment variable.\")\n",
    "        \n",
    "        self.base_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.default_model = \"x-ai/grok-4.1-fast\"  # Can be changed\n",
    "    \n",
    "    def generate(self, prompt: str, model: Optional[str] = None, \n",
    "                temperature: float = 0.7, max_tokens: int = 2000) -> str:\n",
    "        \"\"\"\n",
    "        Generate completion via OpenRouter.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Input prompt\n",
    "            model: Model to use (default: claude-3.5-sonnet)\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://dyk-health-insights.com\",  # Optional but recommended\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model or self.default_model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(self.base_url, headers=headers, json=data, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"OpenRouter API error: {e}\")\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                print(f\"Response: {e.response.text}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class InsightGenerator:\n",
    "    \"\"\"Main insight generation orchestrator.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None, \n",
    "                 pubmed_email: Optional[str] = None,\n",
    "                 pubmed_api_key: Optional[str] = None,\n",
    "                 model: str = \"anthropic/claude-3.5-sonnet\"):\n",
    "        \"\"\"\n",
    "        Initialize insight generator.\n",
    "        \n",
    "        Args:\n",
    "            api_key: OpenRouter API key\n",
    "            pubmed_email: Email for PubMed API\n",
    "            pubmed_api_key: Optional PubMed API key for higher rate limits\n",
    "            model: LLM model to use\n",
    "        \"\"\"\n",
    "        self.llm = OpenRouterClient(api_key)\n",
    "        self.evidence_retriever = EvidenceRetriever(pubmed_email, pubmed_api_key)\n",
    "        self.model = model\n",
    "        self.templates = PromptTemplates()\n",
    "    \n",
    "    def generate_pure_llm(self, cohort_spec: Dict[str, Any], \n",
    "                         template_type: str = \"risk_amplification\",\n",
    "                         region: str = \"singapore\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate insight using pure LLM knowledge (no external tools).\n",
    "        \n",
    "        Args:\n",
    "            cohort_spec: Cohort specification with params and description\n",
    "            template_type: Type of insight template\n",
    "            region: Target region\n",
    "            \n",
    "        Returns:\n",
    "            Generated insight dictionary\n",
    "        \"\"\"\n",
    "        cohort_params = cohort_spec['cohort_params']\n",
    "        cohort_description = cohort_spec['description']\n",
    "        \n",
    "        # Generate prompt\n",
    "        prompt = self.templates.pure_llm_insight_generation(\n",
    "            cohort_description=cohort_description,\n",
    "            cohort_params=cohort_params,\n",
    "            region=region,\n",
    "            template_type=template_type\n",
    "        )\n",
    "        \n",
    "        # Add region-specific context\n",
    "        if region == \"singapore\":\n",
    "            prompt += RegionSpecificPrompts.singapore_context()\n",
    "        else:\n",
    "            prompt += RegionSpecificPrompts.global_context()\n",
    "        \n",
    "        # Generate\n",
    "        try:\n",
    "            response = self.llm.generate(prompt, model=self.model, temperature=0.7)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            insight = self._parse_json_response(response)\n",
    "            \n",
    "            # Add metadata\n",
    "            insight['cohort_id'] = cohort_spec['cohort_id']\n",
    "            insight['cohort_params'] = cohort_params\n",
    "            insight['generation_method'] = 'pure_llm'\n",
    "            insight['model_used'] = self.model\n",
    "            insight['template_type'] = template_type\n",
    "            insight['region'] = region\n",
    "            \n",
    "            return insight\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating pure LLM insight: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_evidence_based(self, cohort_spec: Dict[str, Any],\n",
    "                               template_type: str = \"risk_amplification\",\n",
    "                               region: str = \"singapore\",\n",
    "                               max_sources: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate insight using external evidence (PubMed).\n",
    "        \n",
    "        Args:\n",
    "            cohort_spec: Cohort specification\n",
    "            template_type: Type of insight template\n",
    "            region: Target region\n",
    "            max_sources: Maximum evidence sources to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Generated insight dictionary with evidence\n",
    "        \"\"\"\n",
    "        cohort_params = cohort_spec['cohort_params']\n",
    "        cohort_description = cohort_spec['description']\n",
    "        \n",
    "        # Retrieve evidence\n",
    "        print(f\"Retrieving evidence for: {cohort_description}\")\n",
    "        evidence_data = self.evidence_retriever.retrieve_for_cohort(\n",
    "            cohort_params=cohort_params,\n",
    "            max_sources=max_sources\n",
    "        )\n",
    "        \n",
    "        if not evidence_data['articles']:\n",
    "            print(\"No evidence found, falling back to pure LLM\")\n",
    "            return self.generate_pure_llm(cohort_spec, template_type, region)\n",
    "        \n",
    "        print(f\"Retrieved {evidence_data['total_sources']} sources\")\n",
    "        \n",
    "        # Generate prompt with evidence\n",
    "        prompt = self.templates.evidence_based_insight_generation(\n",
    "            cohort_description=cohort_description,\n",
    "            cohort_params=cohort_params,\n",
    "            evidence_context=evidence_data['evidence_context'],\n",
    "            region=region,\n",
    "            template_type=template_type\n",
    "        )\n",
    "        \n",
    "        # Add region-specific context\n",
    "        if region == \"singapore\":\n",
    "            prompt += RegionSpecificPrompts.singapore_context()\n",
    "        \n",
    "        # Generate\n",
    "        try:\n",
    "            response = self.llm.generate(prompt, model=self.model, temperature=0.6)\n",
    "            \n",
    "            # Parse JSON response\n",
    "            insight = self._parse_json_response(response)\n",
    "            \n",
    "            # Add metadata\n",
    "            insight['cohort_id'] = cohort_spec['cohort_id']\n",
    "            insight['cohort_params'] = cohort_params\n",
    "            insight['generation_method'] = 'evidence_based'\n",
    "            insight['model_used'] = self.model\n",
    "            insight['template_type'] = template_type\n",
    "            insight['region'] = region\n",
    "            insight['evidence_sources'] = evidence_data['articles']\n",
    "            insight['search_queries'] = evidence_data['queries']\n",
    "            \n",
    "            return insight\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating evidence-based insight: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def validate_insight(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate insight using secondary LLM call.\n",
    "        \n",
    "        Args:\n",
    "            insight: Generated insight to validate\n",
    "            \n",
    "        Returns:\n",
    "            Validation results\n",
    "        \"\"\"\n",
    "        prompt = self.templates.validation_prompt(\n",
    "            insight=insight,\n",
    "            cohort_params=insight.get('cohort_params', {})\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.generate(prompt, model=self.model, temperature=0.3)\n",
    "            validation = self._parse_json_response(response)\n",
    "            return validation\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating insight: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def batch_generate(self, cohort_specs: List[Dict[str, Any]],\n",
    "                      method: str = \"pure_llm\",\n",
    "                      insights_per_cohort: int = 3,\n",
    "                      template_types: Optional[List[str]] = None,\n",
    "                      region: str = \"singapore\",\n",
    "                      validate: bool = True,\n",
    "                      rate_limit_delay: float = 1.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Generate insights for multiple cohorts in batch.\n",
    "        \n",
    "        Args:\n",
    "            cohort_specs: List of cohort specifications\n",
    "            method: \"pure_llm\" or \"evidence_based\"\n",
    "            insights_per_cohort: Number of insights to generate per cohort\n",
    "            template_types: List of template types to cycle through\n",
    "            region: Target region\n",
    "            validate: Whether to validate each insight\n",
    "            rate_limit_delay: Delay between API calls (seconds)\n",
    "            \n",
    "        Returns:\n",
    "            List of all generated insights\n",
    "        \"\"\"\n",
    "        if template_types is None:\n",
    "            template_types = [\"risk_amplification\", \"protective_factors\", \n",
    "                            \"behavior_change\", \"early_detection\"]\n",
    "        \n",
    "        all_insights = []\n",
    "        total_cohorts = len(cohort_specs)\n",
    "        \n",
    "        for idx, cohort_spec in enumerate(cohort_specs, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Cohort {idx}/{total_cohorts}: {cohort_spec['description']}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            cohort_insights = []\n",
    "            \n",
    "            for i in range(insights_per_cohort):\n",
    "                template_type = template_types[i % len(template_types)]\n",
    "                \n",
    "                print(f\"  Generating insight {i+1}/{insights_per_cohort} ({template_type})...\")\n",
    "                \n",
    "                # Generate insight\n",
    "                if method == \"pure_llm\":\n",
    "                    insight = self.generate_pure_llm(\n",
    "                        cohort_spec=cohort_spec,\n",
    "                        template_type=template_type,\n",
    "                        region=region\n",
    "                    )\n",
    "                elif method == \"evidence_based\":\n",
    "                    insight = self.generate_evidence_based(\n",
    "                        cohort_spec=cohort_spec,\n",
    "                        template_type=template_type,\n",
    "                        region=region\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown method: {method}\")\n",
    "                \n",
    "                if insight:\n",
    "                    # Validate if requested\n",
    "                    if validate:\n",
    "                        print(\"  Validating...\")\n",
    "                        validation = self.validate_insight(insight)\n",
    "                        if validation:\n",
    "                            insight['validation'] = validation\n",
    "                            print(f\"  Validation score: {validation.get('overall_score', 'N/A')}/100\")\n",
    "                    \n",
    "                    cohort_insights.append(insight)\n",
    "                    all_insights.append(insight)\n",
    "                    print(\"  ✓ Success\")\n",
    "                else:\n",
    "                    print(\"  ✗ Failed\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(rate_limit_delay)\n",
    "            \n",
    "            print(f\"Completed {len(cohort_insights)}/{insights_per_cohort} insights for this cohort\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"BATCH COMPLETE: Generated {len(all_insights)} total insights\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return all_insights\n",
    "    \n",
    "    def _parse_json_response(self, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse JSON from LLM response, handling markdown code blocks.\"\"\"\n",
    "        # Remove markdown code blocks if present\n",
    "        response = response.strip()\n",
    "        if response.startswith(\"```json\"):\n",
    "            response = response[7:]\n",
    "        if response.startswith(\"```\"):\n",
    "            response = response[3:]\n",
    "        if response.endswith(\"```\"):\n",
    "            response = response[:-3]\n",
    "        \n",
    "        response = response.strip()\n",
    "        \n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parse error: {e}\")\n",
    "            print(f\"Response: {response[:500]}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def save_insights(insights: List[Dict[str, Any]], output_path: str):\n",
    "    \"\"\"Save generated insights to JSON file.\"\"\"\n",
    "    # Add timestamp\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    output = {\n",
    "        'generated_at': timestamp,\n",
    "        'total_insights': len(insights),\n",
    "        'insights': insights\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSaved {len(insights)} insights to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2a97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBMED_API_KEY = os.getenv(\"PUBMED_API_KEY\")\n",
    "PUBMED_EMAIL = os.getenv(\"PUBMED_EMAIL\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee1d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator\n",
    "generator = InsightGenerator(api_key=OPENROUTER_API_KEY,\n",
    "                             pubmed_email=PUBMED_EMAIL,\n",
    "                             pubmed_api_key=PUBMED_API_KEY,\n",
    "                             model=\"x-ai/grok-4.1-fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2dc776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define priority_cohorts\n",
    "priority_cohorts = [\n",
    "    {\n",
    "      \"cohort_id\": \"cohort_0025\",\n",
    "      \"cohort_params\": {\n",
    "        \"age_group\": \"18-29\",\n",
    "        \"gender\": \"male\"\n",
    "      },\n",
    "      \"min_insights\": 3,\n",
    "      \"priority_level\": 2,\n",
    "      \"description\": \"18-29 years old, male\"\n",
    "    },\n",
    "    {\n",
    "      \"cohort_id\": \"cohort_0026\",\n",
    "      \"cohort_params\": {\n",
    "        \"age_group\": \"18-29\",\n",
    "        \"gender\": \"female\"\n",
    "      },\n",
    "      \"min_insights\": 3,\n",
    "      \"priority_level\": 2,\n",
    "      \"description\": \"18-29 years old, female\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c17ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Cohort 1/2: 18-29 years old, male\n",
      "================================================================================\n",
      "  Generating insight 1/3 (risk_amplification)...\n",
      "  Validating...\n",
      "  Validation score: 95/100\n",
      "  ✓ Success\n",
      "  Generating insight 2/3 (protective_factors)...\n",
      "  Validating...\n",
      "  Validation score: 98/100\n",
      "  ✓ Success\n",
      "  Generating insight 3/3 (behavior_change)...\n",
      "  Validating...\n",
      "  Validation score: 98/100\n",
      "  ✓ Success\n",
      "Completed 3/3 insights for this cohort\n",
      "\n",
      "================================================================================\n",
      "Cohort 2/2: 18-29 years old, female\n",
      "================================================================================\n",
      "  Generating insight 1/3 (risk_amplification)...\n",
      "  Validating...\n",
      "  Validation score: 95/100\n",
      "  ✓ Success\n",
      "  Generating insight 2/3 (protective_factors)...\n",
      "  Validating...\n",
      "  Validation score: 85/100\n",
      "  ✓ Success\n",
      "  Generating insight 3/3 (behavior_change)...\n",
      "  Validating...\n",
      "  Validation score: 100/100\n",
      "  ✓ Success\n",
      "Completed 3/3 insights for this cohort\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Generated 6 total insights\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "all_insights = generator.batch_generate(priority_cohorts,\n",
    "                                        method=\"pure_llm\",\n",
    "                                        insights_per_cohort=3,\n",
    "                                        template_types=None,\n",
    "                                        region=\"singapore\",\n",
    "                                        validate=True,\n",
    "                                        rate_limit_delay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9ca42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hook': 'Did you know Singaporean men aged 18-29 have nearly 3x higher suicide rates than women their age?',\n",
       "  'explanation': 'Young men in Singapore face high stress from National Service, academics, and early careers, compounded by stigma against seeking help. This elevates suicide risk significantly for this group, making early awareness and support crucial to prevent tragedy.',\n",
       "  'action': 'If feeling stressed or low, call Samaritans of Singapore at 1800-221-4444 for free, confidential 24/7 support—no judgement.',\n",
       "  'source_name': 'Samaritans of Singapore',\n",
       "  'source_url': 'https://www.sos.org.sg/suicide-statistics',\n",
       "  'health_domain': 'mental-health',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': 'nearly 3x higher suicide rates',\n",
       "  'cohort_id': 'cohort_0025',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'male'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'risk_amplification',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 95,\n",
       "   'factual_accuracy': {'score': 95,\n",
       "    'issues': [\"Numeric claim 'nearly 3x higher' is plausible based on Singapore suicide statistics (e.g., males 15-24: ~15-20 per 100k vs. females ~5-7 per 100k in recent years), but exact 18-29 breakdown may vary slightly by year; requires periodic verification.\"]},\n",
       "   'cohort_relevance': {'score': 100, 'issues': []},\n",
       "   'source_faithfulness': {'score': 90,\n",
       "    'issues': [\"Source (Samaritans of Singapore) provides general suicide stats showing male rates 2-3x higher in young adults; insight accurately represents trends but may slightly generalize the 'nearly 3x' for precise 18-29 cohort—confirm exact figures on page.\"]},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'approve',\n",
       "   'revision_suggestions': []}},\n",
       " {'hook': 'Did you know 30 minutes of daily brisk walking can cut type 2 diabetes risk by up to 50%?',\n",
       "  'explanation': 'Young Singaporean men 18-29 face rising prediabetes from sedentary jobs post-NS, family history in Chinese-Malay-Indian groups, and hawker carbs. Regular activity boosts insulin sensitivity, preventing metabolic issues early.',\n",
       "  'action': \"Join HPB's National Steps Challenge: aim for 10,000 steps daily via HDB stairs or MRT walks.\",\n",
       "  'source_name': 'Health Promotion Board (HPB) Singapore',\n",
       "  'source_url': 'https://www.hpb.gov.sg/healthy-living/prevent-diabetes',\n",
       "  'health_domain': 'metabolic',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': 'up to 50% reduction',\n",
       "  'cohort_id': 'cohort_0025',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'male'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'protective_factors',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 98,\n",
       "   'factual_accuracy': {'score': 95,\n",
       "    'issues': [\"Numeric claim 'up to 50%' is plausible based on meta-analyses (e.g., 30-58% risk reduction with regular moderate PA per Diabetes Prevention Program and EPIC studies), but exact phrasing may generalize broader evidence slightly beyond source specifics\"]},\n",
       "   'cohort_relevance': {'score': 100, 'issues': []},\n",
       "   'source_faithfulness': {'score': 95,\n",
       "    'issues': [\"HPB site promotes brisk walking (150 min/week) for diabetes prevention and cites risk reductions, but 'up to 50%' may derive from supporting studies rather than direct quote; attribution appropriate as HPB endorses activity benefits\"]},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'approve',\n",
       "   'revision_suggestions': []}},\n",
       " {'hook': 'Did you know 30 minutes of brisk walking daily can cut your cardiovascular disease risk by 30%?',\n",
       "  'explanation': 'Young Singaporean men aged 18-29 often have sedentary lifestyles from desk jobs, studying, or gaming. WHO data shows regular moderate physical activity reduces CVD risk by ~30%, essential as heart disease rates rise among men here despite low current prevalence.',\n",
       "  'action': \"Brisk walk to your MRT station or take HDB stairs daily for 30 minutes. Join HPB's National Steps Challenge to track and stay motivated.\",\n",
       "  'source_name': 'World Health Organization (WHO)',\n",
       "  'source_url': 'https://www.who.int/news-room/fact-sheets/detail/physical-activity',\n",
       "  'health_domain': 'cardiovascular',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': '30% reduction',\n",
       "  'cohort_id': 'cohort_0025',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'male'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'behavior_change',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 98,\n",
       "   'factual_accuracy': {'score': 100, 'issues': []},\n",
       "   'cohort_relevance': {'score': 95,\n",
       "    'issues': ['Insight is generally relevant but CVD prevalence is very low in this young cohort; prevention focus is still beneficial']},\n",
       "   'source_faithfulness': {'score': 100, 'issues': []},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'approve',\n",
       "   'revision_suggestions': []}},\n",
       " {'hook': 'Did you know young Singaporean women aged 18-29 have nearly 2x the risk of mood disorders than young men?',\n",
       "  'explanation': 'Singapore Mental Health Study data show females face higher depression/anxiety rates due to hormonal factors, academic/work stress, social media pressures, and societal expectations in fast-paced Singapore life. Early action prevents impacts on studies, career, relationships (48 words).',\n",
       "  'action': 'Book free mental health screening at polyclinic or call CHAT Helpline (1800-HELPSU) for confidential youth support.',\n",
       "  'source_name': 'Singapore Medical Journal (Subramaniam et al., 2019)',\n",
       "  'source_url': 'https://www.smj.org.sg/article/role-gender-prevalence-common-mental-disorders-singapore',\n",
       "  'health_domain': 'mental-health',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': 'nearly 2x higher risk',\n",
       "  'cohort_id': 'cohort_0026',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'female'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'risk_amplification',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 95,\n",
       "   'factual_accuracy': {'score': 95,\n",
       "    'issues': [\"Numeric claim 'nearly 2x' aligns with source OR of 1.97-2.27 for mood disorders in females overall; subgroup 18-29 shows similar trend (e.g., 12-month prevalence ~2-3x in young adults per tables), medically plausible and contextualized\"]},\n",
       "   'cohort_relevance': {'score': 100, 'issues': []},\n",
       "   'source_faithfulness': {'score': 95,\n",
       "    'issues': ['Insight accurately reflects source data on higher female prevalence (OR ~2x for mood disorders); attribution correct, though explanation reasons (hormonal, etc.) are interpretive/general, not directly from source']},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'approve',\n",
       "   'revision_suggestions': []}},\n",
       " {'hook': 'Did you know that 30 minutes of daily moderate exercise can reduce depression and anxiety symptoms by 20-30%?',\n",
       "  'explanation': 'Young women aged 18-29 in Singapore face rising stress from studies, work, and urban lifestyles, contributing to higher mental health risks. WHO evidence shows regular activity releases endorphins, improves sleep, and protects brain health, making it a key protective factor for this cohort.',\n",
       "  'action': \"Join HPB's National Steps Challenge via the app – aim for 10,000 steps daily by walking to the MRT or around your HDB estate.\",\n",
       "  'source_name': 'World Health Organization (WHO)',\n",
       "  'source_url': 'https://www.who.int/news-room/fact-sheets/detail/physical-activity',\n",
       "  'health_domain': 'mental-health',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': '20-30% reduction',\n",
       "  'cohort_id': 'cohort_0026',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'female'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'protective_factors',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 85,\n",
       "   'factual_accuracy': {'score': 90,\n",
       "    'issues': [\"Numeric claim of 20-30% reduction is plausible based on meta-analyses (e.g., effect sizes equivalent to moderate symptom reduction), but not universally precise for '30 minutes daily moderate exercise'; general benefits accurate.\"]},\n",
       "   'cohort_relevance': {'score': 100, 'issues': []},\n",
       "   'source_faithfulness': {'score': 50,\n",
       "    'issues': [\"WHO fact sheet confirms physical activity reduces depression/anxiety but lacks specific '20-30% reduction' or '30 minutes' claim; explanation's general benefits align loosely, but numeric not preserved from source.\"]},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'revise',\n",
       "   'revision_suggestions': [\"Remove or generalize numeric claim (e.g., 'significantly reduce') and cite a study/meta-analysis for quantification.\",\n",
       "    'Ensure source directly supports key claims or use a more precise reference for mental health effects.']}},\n",
       " {'hook': 'Did you know one fewer sugary drink daily can cut type 2 diabetes risk by 26%?',\n",
       "  'explanation': 'Young Singaporean women aged 18-29 often consume sugary drinks like bubble tea at hawker centres, raising metabolic risks amid rising youth diabetes rates. Meta-analysis of prospective studies links each daily serving to 26% higher risk, making small cuts impactful for long-term health.',\n",
       "  'action': 'At your next hawker centre or MRT stop, swap bubble tea or sweetened teh for plain water or unsweetened lime juice.',\n",
       "  'source_name': 'Malik et al., Diabetes Care',\n",
       "  'source_url': 'https://diabetesjournals.org/care/article/33/11/2477/39561/Sugar-Sweetened-Beverages-and-Risk-of-Metabolic',\n",
       "  'health_domain': 'metabolic',\n",
       "  'confidence': 'high',\n",
       "  'numeric_claim': '26% lower risk',\n",
       "  'cohort_id': 'cohort_0026',\n",
       "  'cohort_params': {'age_group': '18-29', 'gender': 'female'},\n",
       "  'generation_method': 'pure_llm',\n",
       "  'model_used': 'x-ai/grok-4.1-fast',\n",
       "  'template_type': 'behavior_change',\n",
       "  'region': 'singapore',\n",
       "  'validation': {'overall_score': 100,\n",
       "   'factual_accuracy': {'score': 100, 'issues': []},\n",
       "   'cohort_relevance': {'score': 100, 'issues': []},\n",
       "   'source_faithfulness': {'score': 100, 'issues': []},\n",
       "   'safety': {'score': 100, 'issues': []},\n",
       "   'actionability': {'score': 100, 'issues': []},\n",
       "   'recommendation': 'approve',\n",
       "   'revision_suggestions': []}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b0cb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = generator.validate_insight(insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "afb1b3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_score': 100,\n",
       " 'factual_accuracy': {'score': 100, 'issues': []},\n",
       " 'cohort_relevance': {'score': 100, 'issues': []},\n",
       " 'source_faithfulness': {'score': 100, 'issues': []},\n",
       " 'safety': {'score': 100, 'issues': []},\n",
       " 'actionability': {'score': 100, 'issues': []},\n",
       " 'recommendation': 'approve',\n",
       " 'revision_suggestions': []}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8f117e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = generator.validate_insight(insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70a25a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_score': 98,\n",
       " 'factual_accuracy': {'score': 98,\n",
       "  'issues': [\"Hook phrasing 'smoking in your 40s nearly doubles' slightly interpretive; study shows current frequent smoking associated with ~73-95% increased odds in adults/males, including 40-49 age group, but not specifically causation from smoking starting/in 40s. Still medically accurate and plausible.\"]},\n",
       " 'cohort_relevance': {'score': 100, 'issues': []},\n",
       " 'source_faithfulness': {'score': 100, 'issues': []},\n",
       " 'safety': {'score': 100, 'issues': []},\n",
       " 'actionability': {'score': 100, 'issues': []},\n",
       " 'recommendation': 'approve',\n",
       " 'revision_suggestions': []}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d94056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    {\n",
    "      \"cohort_id\": \"cohort_10019\",\n",
    "      \"cohort_params\": {\n",
    "        \"chronic_conditions\": \"hypertension\"\n",
    "      },\n",
    "      \"min_insights\": 5,\n",
    "      \"priority_level\": 1,\n",
    "      \"description\": \"hypertension\"\n",
    "    },\n",
    "    {\n",
    "      \"cohort_id\": \"cohort_10020\",\n",
    "      \"cohort_params\": {\n",
    "        \"chronic_conditions\": \"high-cholesterol\"\n",
    "      },\n",
    "      \"min_insights\": 5,\n",
    "      \"priority_level\": 2,\n",
    "      \"description\": \"high cholesterol\"\n",
    "    },\n",
    "    {"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed75dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validation Layer\n",
    "Ensures insights conform to schema, have valid sources, and meet quality standards.\n",
    "\"\"\"\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from difflib import SequenceMatcher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97704c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsightValidator:\n",
    "    \"\"\"Validator for DYK insights.\"\"\"\n",
    "    \n",
    "    # Whitelisted domains for sources\n",
    "    WHITELISTED_DOMAINS = {\n",
    "        'who.int', 'cdc.gov', 'nih.gov', 'pubmed.ncbi.nlm.nih.gov',\n",
    "        'thelancet.com', 'nejm.org', 'bmj.com', 'jamanetwork.com',\n",
    "        'healthhub.sg', 'moh.gov.sg', 'hpb.gov.sg'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initialize validator.\n",
    "        \n",
    "        Args:\n",
    "            config: Optional configuration with custom rules\n",
    "        \"\"\"\n",
    "        self.config = config or {}\n",
    "        self.validation_results = []\n",
    "    \n",
    "    def validate_insight(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Comprehensive validation of a single insight.\n",
    "        \n",
    "        Args:\n",
    "            insight: Insight dictionary to validate\n",
    "            \n",
    "        Returns:\n",
    "            Validation result with scores and issues\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'insight_id': insight.get('cohort_id', 'unknown'),\n",
    "            'overall_valid': True,\n",
    "            'issues': [],\n",
    "            'warnings': [],\n",
    "            'scores': {}\n",
    "        }\n",
    "        \n",
    "        # 1. Schema conformity\n",
    "        schema_result = self._validate_schema(insight)\n",
    "        results['scores']['schema'] = schema_result['score']\n",
    "        if schema_result['issues']:\n",
    "            results['issues'].extend(schema_result['issues'])\n",
    "            results['overall_valid'] = False\n",
    "        \n",
    "        # 2. Source verification\n",
    "        source_result = self._validate_source(insight)\n",
    "        results['scores']['source'] = source_result['score']\n",
    "        if source_result['issues']:\n",
    "            results['issues'].extend(source_result['issues'])\n",
    "        if source_result['warnings']:\n",
    "            results['warnings'].extend(source_result['warnings'])\n",
    "        \n",
    "        # 3. Numeric plausibility\n",
    "        numeric_result = self._validate_numeric_claims(insight)\n",
    "        results['scores']['numeric'] = numeric_result['score']\n",
    "        if numeric_result['issues']:\n",
    "            results['issues'].extend(numeric_result['issues'])\n",
    "        \n",
    "        # 4. Content quality\n",
    "        quality_result = self._validate_content_quality(insight)\n",
    "        results['scores']['quality'] = quality_result['score']\n",
    "        if quality_result['issues']:\n",
    "            results['issues'].extend(quality_result['issues'])\n",
    "        if quality_result['warnings']:\n",
    "            results['warnings'].extend(quality_result['warnings'])\n",
    "        \n",
    "        # Calculate overall score\n",
    "        scores = list(results['scores'].values())\n",
    "        results['overall_score'] = sum(scores) / len(scores) if scores else 0\n",
    "        \n",
    "        # Mark as invalid if overall score is too low\n",
    "        if results['overall_score'] < 60:\n",
    "            results['overall_valid'] = False\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _validate_schema(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate schema conformity.\"\"\"\n",
    "        issues = []\n",
    "        score = 100\n",
    "        \n",
    "        # Required fields\n",
    "        required_fields = ['hook', 'explanation', 'action', 'cohort_params']\n",
    "        for field in required_fields:\n",
    "            if field not in insight or not insight[field]:\n",
    "                issues.append(f\"Missing required field: {field}\")\n",
    "                score -= 25\n",
    "        \n",
    "        # Check field types\n",
    "        if 'cohort_params' in insight and not isinstance(insight['cohort_params'], dict):\n",
    "            issues.append(\"cohort_params must be a dictionary\")\n",
    "            score -= 10\n",
    "        \n",
    "        # Check field lengths\n",
    "        if 'hook' in insight:\n",
    "            hook_words = len(insight['hook'].split())\n",
    "            if hook_words > 25:\n",
    "                issues.append(f\"Hook too long: {hook_words} words (max 25)\")\n",
    "                score -= 10\n",
    "            if hook_words < 5:\n",
    "                issues.append(f\"Hook too short: {hook_words} words (min 5)\")\n",
    "                score -= 10\n",
    "        \n",
    "        if 'explanation' in insight:\n",
    "            exp_words = len(insight['explanation'].split())\n",
    "            if exp_words < 30 or exp_words > 80:\n",
    "                issues.append(f\"Explanation length suboptimal: {exp_words} words (target 40-60)\")\n",
    "                score -= 5\n",
    "        \n",
    "        if 'action' in insight:\n",
    "            action_words = len(insight['action'].split())\n",
    "            if action_words > 40:\n",
    "                issues.append(f\"Action too long: {action_words} words (max 40)\")\n",
    "                score -= 10\n",
    "        \n",
    "        return {'score': max(0, score), 'issues': issues}\n",
    "    \n",
    "    def _validate_source(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate source credibility and accessibility.\"\"\"\n",
    "        issues = []\n",
    "        warnings = []\n",
    "        score = 100\n",
    "        \n",
    "        source_url = insight.get('source_url', '')\n",
    "        source_name = insight.get('source_name', '')\n",
    "        \n",
    "        if not source_name:\n",
    "            issues.append(\"Missing source name\")\n",
    "            score -= 30\n",
    "        \n",
    "        if not source_url or source_url == 'general medical knowledge':\n",
    "            warnings.append(\"No specific source URL provided\")\n",
    "            score -= 10\n",
    "        else:\n",
    "            # Validate URL format\n",
    "            try:\n",
    "                parsed = urlparse(source_url)\n",
    "                if not parsed.scheme or not parsed.netloc:\n",
    "                    issues.append(f\"Invalid URL format: {source_url}\")\n",
    "                    score -= 20\n",
    "                else:\n",
    "                    # Check domain whitelist\n",
    "                    domain = parsed.netloc.lower()\n",
    "                    # Remove www. prefix\n",
    "                    domain = domain.replace('www.', '')\n",
    "                    \n",
    "                    if not any(domain.endswith(whitelist) for whitelist in self.WHITELISTED_DOMAINS):\n",
    "                        warnings.append(f\"Source domain not in whitelist: {domain}\")\n",
    "                        score -= 10\n",
    "            \n",
    "            except Exception as e:\n",
    "                issues.append(f\"Error parsing URL: {e}\")\n",
    "                score -= 20\n",
    "        \n",
    "        # Check source tier if available\n",
    "        source_tier = insight.get('source_tier')\n",
    "        if source_tier and source_tier > 3:\n",
    "            warnings.append(f\"Low-tier source (tier {source_tier})\")\n",
    "            score -= 5\n",
    "        \n",
    "        return {'score': max(0, score), 'issues': issues, 'warnings': warnings}\n",
    "    \n",
    "    def _validate_numeric_claims(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate plausibility of numeric claims.\"\"\"\n",
    "        issues = []\n",
    "        score = 100\n",
    "        \n",
    "        # Extract numeric claims from text\n",
    "        full_text = f\"{insight.get('hook', '')} {insight.get('explanation', '')} {insight.get('action', '')}\"\n",
    "        \n",
    "        # Find percentages\n",
    "        percentages = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*%', full_text)\n",
    "        for pct in percentages:\n",
    "            pct_val = float(pct)\n",
    "            if pct_val > 100:\n",
    "                issues.append(f\"Implausible percentage: {pct_val}%\")\n",
    "                score -= 15\n",
    "            if pct_val > 95:\n",
    "                issues.append(f\"Very high percentage (verify): {pct_val}%\")\n",
    "                score -= 5\n",
    "        \n",
    "        # Find risk multipliers (e.g., \"3x higher\", \"5 times\")\n",
    "        multipliers = re.findall(r'(\\d+(?:\\.\\d+)?)\\s*(?:x|times)\\s+(?:higher|greater|more|increased)', full_text.lower())\n",
    "        for mult in multipliers:\n",
    "            mult_val = float(mult)\n",
    "            if mult_val > 20:\n",
    "                issues.append(f\"Implausibly high risk multiplier: {mult_val}x\")\n",
    "                score -= 15\n",
    "        \n",
    "        # Check for absolute risk numbers\n",
    "        absolute_risks = re.findall(r'(\\d+)\\s+in\\s+(\\d+)', full_text)\n",
    "        for numerator, denominator in absolute_risks:\n",
    "            num, denom = int(numerator), int(denominator)\n",
    "            if num > denom:\n",
    "                issues.append(f\"Invalid ratio: {num} in {denom}\")\n",
    "                score -= 20\n",
    "        \n",
    "        return {'score': max(0, score), 'issues': issues}\n",
    "    \n",
    "    def _validate_content_quality(self, insight: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate content quality and appropriateness.\"\"\"\n",
    "        issues = []\n",
    "        warnings = []\n",
    "        score = 100\n",
    "        \n",
    "        hook = insight.get('hook', '').lower()\n",
    "        explanation = insight.get('explanation', '').lower()\n",
    "        action = insight.get('action', '').lower()\n",
    "        \n",
    "        # Check if hook starts with \"Did you know\"\n",
    "        if not hook.startswith('did you know'):\n",
    "            warnings.append(\"Hook should start with 'Did you know'\")\n",
    "            score -= 5\n",
    "        \n",
    "        # Check for fear-mongering language\n",
    "        fear_words = ['deadly', 'fatal', 'dangerous', 'terrifying', 'horrifying']\n",
    "        for word in fear_words:\n",
    "            if word in hook or word in explanation:\n",
    "                warnings.append(f\"Potentially fear-mongering language: '{word}'\")\n",
    "                score -= 5\n",
    "        \n",
    "        # Check for vague language\n",
    "        vague_phrases = ['may', 'might', 'could possibly', 'perhaps', 'some studies suggest']\n",
    "        vague_count = sum(1 for phrase in vague_phrases if phrase in explanation)\n",
    "        if vague_count > 2:\n",
    "            warnings.append(\"Too much hedging/vague language\")\n",
    "            score -= 10\n",
    "        \n",
    "        # Check for actionability\n",
    "        action_verbs = ['speak', 'talk', 'consult', 'ask', 'schedule', 'start', 'try', \n",
    "                       'consider', 'visit', 'contact', 'discuss', 'check', 'get']\n",
    "        has_action_verb = any(verb in action for verb in action_verbs)\n",
    "        if not has_action_verb:\n",
    "            issues.append(\"Action lacks clear action verb\")\n",
    "            score -= 15\n",
    "        \n",
    "        # Check for medical diagnosis/treatment claims\n",
    "        diagnosis_words = ['diagnose', 'treat', 'cure', 'prescribe']\n",
    "        for word in diagnosis_words:\n",
    "            if word in action:\n",
    "                issues.append(f\"Action should not include medical diagnosis/treatment: '{word}'\")\n",
    "                score -= 20\n",
    "        \n",
    "        # Check cohort specificity\n",
    "        cohort_params = insight.get('cohort_params', {})\n",
    "        full_text = f\"{hook} {explanation} {action}\".lower()\n",
    "        \n",
    "        # Should mention cohort characteristics\n",
    "        cohort_mentions = 0\n",
    "        if 'age_group' in cohort_params:\n",
    "            age = cohort_params['age_group']\n",
    "            if age in full_text or 'age' in full_text:\n",
    "                cohort_mentions += 1\n",
    "        \n",
    "        if 'gender' in cohort_params:\n",
    "            gender = cohort_params['gender']\n",
    "            if gender in full_text:\n",
    "                cohort_mentions += 1\n",
    "        \n",
    "        if cohort_mentions == 0 and len(cohort_params) > 1:\n",
    "            warnings.append(\"Insight doesn't reference specific cohort characteristics\")\n",
    "            score -= 10\n",
    "        \n",
    "        return {'score': max(0, score), 'issues': issues, 'warnings': warnings}\n",
    "    \n",
    "    def validate_batch(self, insights: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate a batch of insights and provide summary statistics.\n",
    "        \n",
    "        Args:\n",
    "            insights: List of insights to validate\n",
    "            \n",
    "        Returns:\n",
    "            Summary validation results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for insight in insights:\n",
    "            result = self.validate_insight(insight)\n",
    "            results.append(result)\n",
    "        \n",
    "        # Summary statistics\n",
    "        valid_count = sum(1 for r in results if r['overall_valid'])\n",
    "        avg_score = sum(r['overall_score'] for r in results) / len(results) if results else 0\n",
    "        \n",
    "        # Count issues by type\n",
    "        all_issues = []\n",
    "        all_warnings = []\n",
    "        for r in results:\n",
    "            all_issues.extend(r['issues'])\n",
    "            all_warnings.extend(r['warnings'])\n",
    "        \n",
    "        summary = {\n",
    "            'total_insights': len(insights),\n",
    "            'valid_insights': valid_count,\n",
    "            'invalid_insights': len(insights) - valid_count,\n",
    "            'average_score': round(avg_score, 2),\n",
    "            'total_issues': len(all_issues),\n",
    "            'total_warnings': len(all_warnings),\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def check_duplicates(self, insights: List[Dict[str, Any]], \n",
    "                        threshold: float = 0.85) -> List[Tuple[int, int, float]]:\n",
    "        \"\"\"\n",
    "        Check for duplicate or highly similar insights using semantic similarity.\n",
    "        \n",
    "        Args:\n",
    "            insights: List of insights\n",
    "            threshold: Similarity threshold (0-1) above which insights are considered duplicates\n",
    "            \n",
    "        Returns:\n",
    "            List of (index1, index2, similarity_score) tuples for duplicates\n",
    "        \"\"\"\n",
    "        duplicates = []\n",
    "        \n",
    "        for i in range(len(insights)):\n",
    "            for j in range(i + 1, len(insights)):\n",
    "                # Compare full text\n",
    "                text1 = f\"{insights[i].get('hook', '')} {insights[i].get('explanation', '')}\"\n",
    "                text2 = f\"{insights[j].get('hook', '')} {insights[j].get('explanation', '')}\"\n",
    "                \n",
    "                similarity = SequenceMatcher(None, text1, text2).ratio()\n",
    "                \n",
    "                if similarity >= threshold:\n",
    "                    duplicates.append((i, j, similarity))\n",
    "        \n",
    "        return duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
